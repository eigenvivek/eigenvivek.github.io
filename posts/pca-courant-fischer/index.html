<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Deriving PCA from the Courant–Fischer Theorem</title> <header> <div class=blog-name ><a href="">Vivek Gopalakrishnan</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/posts/">Posts</a> <li><a href="/talks/">Talks</a> </ul> <img src="/assets/flavicon.ico" id=menu-icon > </nav> </header> <div class=franklin-content ><h1 id=deriving_pca_from_the_courantfischer_theorem ><a href="#deriving_pca_from_the_courantfischer_theorem" class=header-anchor >Deriving PCA from the Courant–Fischer Theorem</a></h1> <p>2021-09-06 | <em>A variational eigenvalue characterization of principle components</em></p> <p>Principal Component Analysis &#40;PCA&#41; is one of my favorite algorithms because &#40;1&#41; it is foundational and broadly applicable, and &#40;2&#41; it can be derived using approaches from many different mathematical fields.<sup id="fnref:1"><a href="#fndef:1" class=fnref >[1]</a></sup></p> <p><table class=fndef  id="fndef:1"> <tr> <td class=fndef-backref ><a href="#fnref:1">[1]</a> <td class=fndef-content >The second chapter of <a href="https://link.springer.com/content/pdf/10.1007/978-0-387-87811-9.pdf">Vidal et al. &#40;2015&#41;</a> presents PCA from three different perspectives: statistical, geometric, and rank-minimization. </table> Typically, these derivations involve optimizing a high-dimensional objective function with Lagrange multipliers. Instead, the Courant–Fischer Theorem, a fundamental theorem from linear algebra, can be used to more simply arrive at a solution to this optimization problem.<sup id="fnref:2"><a href="#fndef:2" class=fnref >[2]</a></sup></p> <p><table class=fndef  id="fndef:2"> <tr> <td class=fndef-backref ><a href="#fnref:2">[2]</a> <td class=fndef-content >This post synthesizes ideas I encountered in Matrix Analysis &#40;<a href="https://www.amazon.com/Matrix-Analysis-Second-Roger-Horn/dp/0521548233">JHU.AMS.792</a>&#41; and Unsupervised Learning &#40;<a href="http://www.vision.jhu.edu/teaching/learning/learning17/">JHU.BME.692</a>&#41;. </table> In this post, we prove the Courant–Fischer Theorem, use it to derive PCA, and finally consider some technical points from the derivation.</p> <h2 id=formulating_pca_as_a_matrix_optimization_problem ><a href="#formulating_pca_as_a_matrix_optimization_problem" class=header-anchor >Formulating PCA as a matrix optimization problem</a></h2> <p>One view of PCA can be motivated by the following question:</p> <blockquote> <p>Given a set of high-dimensional data points, what are the <strong>directions of maximal variance?</strong></p> </blockquote> <p>Represent one such high-dimensional data point with a random vector \(x \in \mathbb{R}^D\) and assume the following:</p> <ul> <li><p>\(\mathbb{E}[x] = 0\) &#40;i.e., the vector is <em>mean-centered</em>&#41;, and</p> <li><p>\(\mathrm{rank}(\Sigma_x) \geq d\), where \(\Sigma_x\) is the covariance matrix of \(x\) and \(d \ll D\) is the dimensionality of our embedding.</p> </ul> <p>These assumptions lead us to the following definition:</p> <p><strong>Definition 1 &#40;Principal Components of \(x\)&#41;.</strong> The \(d\) principal components &#40;PCs&#41; of \(x\) are a set of mutually uncorrelated<sup id="fnref:3"><a href="#fndef:3" class=fnref >[3]</a></sup> random variables \(\vec{y} \in \mathbb{R}^d\) defined as</p> \[y_i = \langle u_i, x \rangle \in \mathbb{R},\quad u_i \in \mathbb{R}^D \text{ and } \lVert u_i \rVert_2 = 1,\quad i=1,\dots,d \,,\] <p>such that the variance of \(y_i\) is maximized subject to</p> \[\mathrm{Var}(y_1) \geq \cdots \geq \mathrm{Var}(y_d) > 0 \,.\] <p><table class=fndef  id="fndef:3"> <tr> <td class=fndef-backref ><a href="#fnref:3">[3]</a> <td class=fndef-content >That is, \(\mathrm{cov}(y_i, y_j) = 0\) for all \(i \neq j\). </table> We can derive an important characterization of the PCs from this definition:</p> <p><strong>Lemma 1.</strong> \(\mathrm{Var}(y_i) = u_i^*\Sigma_x u_i \,.\) <br /><em>Proof:</em> This follows from the definition of variance:</p> <div class=nonumber >\[\begin{aligned} \mathrm{Var}(y_i) &= \mathbb{E}[y_i^2] - \cancel{\mathbb{E}[y_i]^2} &&\text{(Given $\mathbb{E}[x] = 0$.)} \\ &= \mathbb{E}[u_i^* x u_i^* x] &&\text{(By definition of $y_i$.)} \\ &= \mathbb{E}[u_i^* x x^* u_i] &&\text{(The dot product is commutative.)} \\ &= u_i^* \mathbb{E}[x x^*] u_i &&\text{(Expectation is linear.)} \\ &= u_i^*\Sigma_x u_i \,. \end{aligned}\]</div> <p>Therefore, we are looking for the vectors \(u_i\) that maximize this <a href="https://en.wikipedia.org/wiki/Quadratic_form">quadratic form</a>&#33;</p> <p>We need one final result before we can apply the Courant–Fischer Theorem:</p> <p><strong>Lemma 2.</strong> The covariance matrix of a random variable is Hermitian.<sup id="fnref:4"><a href="#fndef:4" class=fnref >[4]</a></sup> <br /><em>Proof:</em></p> \[\Sigma_x^* = (\mathbb{E}[xx^*] - \mathbb{E}[x]\mathbb{E}[x^*])^* = \mathbb{E}[(xx^*)^*] - \mathbb{E}[x^*]\mathbb{E}[(x^*)^*] = \Sigma_x \,.\] <table class=fndef  id="fndef:4"> <tr> <td class=fndef-backref ><a href="#fnref:4">[4]</a> <td class=fndef-content >A matrix \(A\) is Hermitian if \(A = A^*\) &#40;i.e., a more general form of symmetry that applies to complex matrices&#41;. </table> <h2 id=proving_the_courantfischer_theorem ><a href="#proving_the_courantfischer_theorem" class=header-anchor >Proving the Courant–Fischer Theorem</a></h2> <p>The Courant–Fischer Theorem provides a very useful bound on all possible quadratic forms of a Hermtian matrix in a subspace using the eigenvalues of the matrix.</p> <p><strong>Theorem 1 &#40;Courant–Fischer&#41;.</strong> Let \(A \in M_n\) be a Hermitian matrix with eigenvalues \(\lambda_1 \geq \cdots \geq \lambda_n\) and corresponding eigenvectors \(u_1, \dots, u_n\). Then,</p> \[ \lambda_k = \min_{\substack{\cal{V} \subseteq \mathbb{C}^n \\ \mathrm{dim}\cal{V} = n-k}} \max_{\substack{x \in \cal{V}^\perp \\ x \neq \vec{0}}} \frac{x^* A x}{x^* x} \,. \] <p><em>Proof:</em> Because \(A\) is Hermitian, it is unitarily diagonalizable. Let \(\cal{U} = \mathrm{span}\{ u_1, \dots, u_k \}\). Then, the intersection of \(\cal U\) and \(\cal V^\perp\) has a dimension of at least \(1\).<sup id="fnref:5"><a href="#fndef:5" class=fnref >[5]</a></sup></p> <p><table class=fndef  id="fndef:5"> <tr> <td class=fndef-backref ><a href="#fnref:5">[5]</a> <td class=fndef-content >From the Inclusion-Exclusion Principle: <div class=nonumber >\[\begin{aligned} \dim (\mathcal{U} \cap \mathcal{V^\perp}) &= \dim \cal U + \dim \cal V^\perp - \dim(\cal U \cup \cal V^\perp) \\ &= k + (n - k + 1) - \dim(\cal U \cup \cal V^\perp) \\ &\geq k + (n - k + 1) - n \\ &= 1 \,. \end{aligned}\]</div> </table> Let \(w \in \cal U \cap \cal V^\perp\) and express \( w = \sum_{i=1}^k c_i u_i \) &#40;a linear combination of the first \(k\) eigenvectors&#41;. Then,</p> \[ \begin{align*} \frac{w^* A w}{w^* w} = \frac{\sum_{i=1}^k \lambda_i c_i^2}{\sum_{i=1}^k c_i^2} \geq \lambda_k \frac{\sum_{i=1}^k c_i^2}{\sum_{i=1}^k c_i^2} = \lambda_k \,, \end{align*} \] <p>with equality if \(w = u_k \Rightarrow \cal V^\perp = \mathrm{span}\{u_1, \dots, u_{k-1}\} \,.\)</p> <p><strong>Corollary 1.</strong> As a direct consequence of the Courant–Fischer Theorem, for any matrix \(A\) as in Theorem 1,</p> \[ \lambda_k = \max_{x \perp \{u_1, \dots, u_{k-1}\}} \frac{x^* A x}{x^* x} \,. \] <p>This vastly simplifies the constrained optimization problem presented in Lemma 1 – no need for Lagrange multipliers&#33;</p> <h2 id=deriving_pca ><a href="#deriving_pca" class=header-anchor >Deriving PCA</a></h2> <p><strong>Theorem 2 &#40;Principal Component Analysis&#41;.</strong> Given a random variable \(x\) with covariance matrix \(\Sigma_x \in M_n\) with eigenvalues \(\lambda_1 \geq \cdots \geq \lambda_n\) and associated normalized eigenvectors \(u_1, \dots, u_n\), the \(i\)-th principal component of \(x\) is</p> \[ y_i = \langle u_i, x \rangle \,.\] <p><em>Proof:</em> From Lemma 1, we have that \( \mathrm{Var}(y_i) = u_i^* \Sigma_x u_i \text{ with } \lVert u_i \rVert_2 = 1 \). From the Courant–Fischer Theorem, we have that</p> \[ \max_{u \in \mathbb{C}^n} \mathrm{Var}(y_1) = \max_{u \in \mathbb{C}^n} \frac{u^* \Sigma_x u}{u^* u} = \lambda_1 \,.\] <p>That is, the first principal component of \(x\) is in the direction of the eigenvector corresponding to the largest eigenvalue. Recall that, by definition, the principal components are mutually uncorrelated. This implies that, unless \(\Sigma_x\) is the zero matrix, \(\lambda_1 > 0 \Rightarrow u_1 \perp u_2\).<sup id="fnref:6"><a href="#fndef:6" class=fnref >[6]</a></sup></p> <p>Therefore,</p> \[ \max_{u \perp \{u_1\}} \mathrm{Var}(y_2) = \max_{u \perp \{u_1\}} \frac{u^* \Sigma_x u}{u^* u} = \lambda_2 \,.\] <p>By induction, we can show that the uncorrelatedness of the principal components implies the orthogonality of their underlying vectors, continuing to enable use of the Courant–Fischer Theorem. Thus, PCA has been derived&#33;</p> <table class=fndef  id="fndef:6"> <tr> <td class=fndef-backref ><a href="#fnref:6">[6]</a> <td class=fndef-content >By the definition of uncorrelatedness, <div class=nonumber >\[\begin{aligned} 0 &= \mathrm{Cov}(y_1, y_2) \\ &= \mathbb{E}[u_1^* x u_2^* x] \\ &= \mathbb{E}[u_1^* x x^* u_2] \\ &= u_1^* \mathbb{E}[x x^*] u_2 \\ &= u_1^*\Sigma_x u_2 \\ &= (\Sigma_x u_1)^* u_2 \\ &= \lambda_1 u_1^*u_2 \,. \end{aligned}\]</div> </table> <h2 id=technical_considerations ><a href="#technical_considerations" class=header-anchor >Technical Considerations</a></h2> <blockquote> <p>What if \(\lambda_i\) is complex? Which eigenvalue is &quot;largest&quot; in this case?</p> </blockquote> <p>It is true that there is no total ordering on the complex numbers &#40;see <a href="https://math.stackexchange.com/questions/487997/total-ordering-on-complex-numbers">here</a>&#41;, so there would be no &quot;largest&quot; eigenvalue, as such... However, we have shown that \(\Sigma_x\) is Hermitian, so \(\sigma(\Sigma_x) \subseteq \mathbb{R}\,.\)<sup id="fnref:7"><a href="#fndef:7" class=fnref >[7]</a></sup></p> <table class=fndef  id="fndef:7"> <tr> <td class=fndef-backref ><a href="#fnref:7">[7]</a> <td class=fndef-content >This is because Hermitian matrices are unitary diagonalizable, and the diagonal matrix, which is comprised of the eigenvalues, is itself Hermitian. </table> <blockquote> <p>Okay, but what if \(\lambda_i < 0\)? That wouldn&#39;t be a valid variance.</p> </blockquote> <p>Any covariance matrix \(\Sigma_x\) is positive semi-definite,<sup id="fnref:8"><a href="#fndef:8" class=fnref >[8]</a></sup> so we also know that the eigenvalues of \(\Sigma_x\) are non-negative, i.e., \(\sigma(\Sigma_x) \subseteq \mathbb{R}_{\geq 0}\,.\)</p> <table class=fndef  id="fndef:8"> <tr> <td class=fndef-backref ><a href="#fnref:8">[8]</a> <td class=fndef-content >For any vector \(z \neq \vec{0}\), <div class=nonumber >\[\begin{aligned} z^* \Sigma_x z &= \mathbb{E}[z^* xx^* z] - \mathbb{E}[z^* x]\mathbb{E}[x^* z] \\ &= \mathbb{E}[(z^* x)^2] - \mathbb{E}[z^* x]^2 \\ &= \mathrm{V}(z^* x) \\ &\geq 0 \,. \end{aligned}\]</div> </table> <blockquote> <p>What happens if \(\lambda_i = \lambda_j\) for \(i \neq j\)?</p> </blockquote> <p>Recall that \(\Sigma_x\) is Hermitian, and therefore is unitarily diagonalizable. This implies that every eigenvalue of \(\Sigma_x\) has equal geometric and algebraic multiplicities.<sup id="fnref:9"><a href="#fndef:9" class=fnref >[9]</a></sup> Therefore, for repeated eigenvalues, we can pick any orthonormal eigenvectors from the eigenspace of \(\lambda = \lambda_i = \lambda_j\), meaning the PCs are defined only up to a rotation.</p> <table class=fndef  id="fndef:9"> <tr> <td class=fndef-backref ><a href="#fnref:9">[9]</a> <td class=fndef-content >This follows from the Jordan Canonical Form&#33; </table> <div class=page-foot > <div class=copyright > &copy; Vivek Gopakrishnan. Last modified: October 30, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script>