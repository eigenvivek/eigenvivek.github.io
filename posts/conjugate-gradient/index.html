<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>The Conjugate Gradient Method</title> <header> <div class=blog-name ><a href="">Vivek Gopalakrishnan</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/posts/">Posts</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><h1 id=the_conjugate_gradient_method ><a href="#the_conjugate_gradient_method" class=header-anchor >The Conjugate Gradient Method</a></h1> <p>2022-07-06 | <em>Geometric underpinnings, eigenvalues, and the convergence rate</em></p> <p>The goals of this post are:<sup id="fnref:1"><a href="#fndef:1" class=fnref >[1]</a></sup></p> <ol> <li><p>To derive the conjugate gradient method,</p> <li><p>Implement and benchmark various versions with Julia, and</p> <li><p>Explain the connection between eigenvalues and the convergence rate.</p> </ol> <p>The code in this post can be viewed in a Jupyter Notebook <a href="https://nbviewer.org/github/eigenvivek/eigenvivek.github.io/blob/main/_assets/notebooks/conjugate_gradient.ipynb">here</a>.</p> <table class=fndef  id="fndef:1"> <tr> <td class=fndef-backref ><a href="#fnref:1">[1]</a> <td class=fndef-content >Notes from the Medical Vision Group&#39;s summer reading of <em>Numerical Optimization</em> by J. Nocedal, Stephen J. Wright, 1999. </table> <h2 id=setup ><a href="#setup" class=header-anchor >Setup</a></h2> <p>The goal of the conjugate gradient algorithm is to iteratively solve linear systems of the form \(Ax = b \,\) where <strong>we assume that \(A \in M_n(\mathbb R)\) is symmetric positive definite.</strong><sup id="fnref:2"><a href="#fndef:2" class=fnref >[2]</a></sup> It turns out that solving such a linear system is equivalent to minimizing the quadratic form</p> \[f(x) = x^TAx - b^Tx + c \,.\] <p>To see this, look at its gradient:</p> \[\nabla f(x) = Ax - b \,.\] <p>The gradient equals zero exactly at the \(x=x^*\) that minimizes the the residual of the linear system \(r(x^*) = Ax^* - b\,.\)<sup id="fnref:3"><a href="#fndef:3" class=fnref >[3]</a></sup> Additionally, the Hessian matrix of \(f\) is \(A\), which is positive definite, so it has exactly one optimum, which is a minumum&#33;</p> <p><table class=fndef  id="fndef:2"> <tr> <td class=fndef-backref ><a href="#fnref:2">[2]</a> <td class=fndef-content >This may seem to be a strong assumption, but we will soon show how to bypass this for any general rectangular matrix. </table> <table class=fndef  id="fndef:3"> <tr> <td class=fndef-backref ><a href="#fnref:3">[3]</a> <td class=fndef-content >We will write that the \(k\)-th iterate \(x_k\) has a residual of \(r_k = \nabla f(x_k) = Ax_k - b \,.\) </table> To minimize the quadratic form, we <em>could</em> use steepest descent, which is gradient descent where you take the largest step possible to minimize the loss. However, for certain linear systems, gradient descent converges very slowly &#40;<em>slowly</em> here means that the number of steps needed for convergence is \(\gg n\), the dimension of \(A\)&#41;. Instead, conjugate gradient gives an iterative optimization that is guaranteed to converge in at most \(n\) steps...<sup id="fnref:4"><a href="#fndef:4" class=fnref >[4]</a></sup></p> <table class=fndef  id="fndef:4"> <tr> <td class=fndef-backref ><a href="#fnref:4">[4]</a> <td class=fndef-content >...in exact arithmetic – floating point accuracy means it can take a little longer, but it&#39;s still faster than gradient descent. </table> <h2 id=understanding_conjugacy ><a href="#understanding_conjugacy" class=header-anchor >Understanding conjugacy</a></h2> <p>In essence, the conjugate gradient method is simply a change of basis. However, the basis we seek is very special: specifically, we are looking for a set of basis vectors that are <em>conjugate</em> with respect to \(A\). Two vectors \(u\) and \(v\) are conjugate if they satisfy a special kind of orthogonality:</p> \[ u^T A v = v^T A v = 0 \,.\] <p>This means that after \(u\) &#40;or \(v\)&#41; is transformed by \(A\), it is orthogonal to \(v\) &#40;or \(u\)&#41;. This basis is very useful for finding the solution to a linear system, as we&#39;ll show below. First, a quick lemma:</p> <h3 id=linear_independence_of_conjugate_vectors ><a href="#linear_independence_of_conjugate_vectors" class=header-anchor >Linear independence of conjugate vectors</a></h3> <p><strong>Lemma 1.</strong> If \(u\) and \(v\) are conjugate with respect to \(A\), then they are also linearly independent. <br /><em>Proof:</em> Suppose, by way of contradiction, \(u\) and \(v\) are not linearly independent. Then, there exists some non-zero constant \(k\) such that \(u = kv\). This implies \(0 = u^TAv = kv^TAv \Longrightarrow v^TAv = 0\). However, this is a contradiction because \(A\) positive definite means \(v^TAv > 0 \,. \)</p> <p>By induction, we can also show that a set of conjugate vectors are linearly independent.</p> <h3 id=conjugate_directions_method ><a href="#conjugate_directions_method" class=header-anchor >Conjugate directions method</a></h3> <p>Suppose that we have a conjugate basis \(\{p_1, \dots, p_n\}\) with respect to \(A\). Since these vectors are linearly independent, we can express \(x^*\) as</p> \[ x^* = \alpha_1 p_1 + \cdots + \alpha_n p_n \Longrightarrow Ax^* = b = \alpha_1 Ap_1 + \cdots + \alpha_n Ap_n \,. \] <p>Premultiplying by the vector \(p_k\), we see that</p> \[ p_k^T b = \alpha_k p_k^T A p_k \,, \] <p>since the other terms cancel out by conjugacy&#33; Therefore, we have that the coefficients are</p> \[ \alpha_k = \frac{p_k^T b}{p_k^T A p_k} \,, \] <p>which are all quantities we know how to compute. That is, changing our basis to a conjugate one makes it very easy to solve a linear system.</p> <p>This simple results tells us two important facts:</p> <ol> <li><p>If we have a procedure that produces a conjugate basis vector at each step, we can solve a linear system in at most \(n\) steps.</p> <li><p>If we have a set of conjugate basis vectors for \(A\), it is trivial to solve a linear system. The brilliance of the conjugate gradient method is in how we find these vectors.</p> </ol> <h3 id=visualizing_conjugate_directions_and_condition_number ><a href="#visualizing_conjugate_directions_and_condition_number" class=header-anchor >Visualizing conjugate directions and condition number</a></h3> <p>Before discussing the generating procedure, it&#39;s useful to visualize conjugacy and our loss landscape.</p> <p>Since we are working with positive definite matrices, it&#39;s useful to have a function to randomly generate them. The procedure we use leverages the fact that \(A^T A\) is gauranteed to be a positive semidefinite matrix &#40;use the definition of positive semidefiniteness to prove this&#41;.<sup id="fnref:5"><a href="#fndef:5" class=fnref >[5]</a></sup> Therefore, we almost always generate a positive definite matrix by samping a random square matrix and premultiplying it by its transpose.</p> <pre><code class=language-julia >function random_psd&#40;n::Int64&#61;2&#41;
    A &#61; randn&#40;n, 2&#41;
    return A&#39; * A
end</code></pre> <p><table class=fndef  id="fndef:5"> <tr> <td class=fndef-backref ><a href="#fnref:5">[5]</a> <td class=fndef-content >Note that \(A^T A\) is positive semidefinite if and only if \(\mathrm{rank}(A) < n\), and the probability of this happening is very small. </table> Next, we plot the level sets of the quadratic form defined by our positive definite \(A\). That is, we visualize multiple elliptical curves along which \(x^TAx = c\) for some constant \(c\). These let us visualize the loss landscape we are trying to optimize.</p> <p>The final concept we will discuss with these curves is the <strong>condition number</strong>. The condition number is the ratio between the largest and smallest eigenvalues. When this number is small &#40;i.e., closer to 1&#41;, the closer the ellipses are to a circle &#40;<em>left</em>&#41;. This corresponds to a system that is more amenable to gradient descent: you can pick any direction to descent and make good progress. For a system with a large condition number &#40;<em>right</em>&#41;, some directions are much more fruitful than others. This means gradient descent can take a very long time if you choose a poor starting point for the optimization.</p> <p><img src="../../assets/images/conjgrad_ellipses.svg" alt=conjugate-gradient-ellipses  /></p> <h2 id=deriving_the_conjugate_gradient_method ><a href="#deriving_the_conjugate_gradient_method" class=header-anchor >Deriving the conjugate gradient method</a></h2> <p>Here, we derive conjugate gradient as an iterative method. Note, the first search direction \(p_0 = -r_0\) since it is the negative gradient.</p> <h3 id=finding_the_optimal_step_size_alpha_k ><a href="#finding_the_optimal_step_size_alpha_k" class=header-anchor >Finding the optimal step size \(\alpha_k\)</a></h3> <p>Assume we start at some point \(x_0\). For a set of conjugate directions \(\{p_0, \dots, p_k\}\), we define the update function as</p> \[ x_{k+1} = x_k + \alpha_k p_k \,, \quad\quad\text{(Eq. 1)}\] <p>where \(\alpha_k\) is the length that optimally descends \(p_k\). To find \(\alpha_k\), we define \(g(\alpha_k) = f(x_k+\alpha_kp_k)\), so that when</p> \[ \frac{\partial g}{\partial \alpha_k} = (x_k + \alpha_kp_k)^TAp_k - b^Tp_k = x_k^TAp_k + \alpha_kp_k^TAp_k - b^Tp_k = 0 \,, \] <p>we have that</p> \[\alpha_k = \frac{p_k^T(b-Ax_k)}{p_k^TAp_k} = -\frac{p_k^Tr_k}{p_k^TAp_k} \,. \] <h3 id=finding_the_next_search_direction_p_k ><a href="#finding_the_next_search_direction_p_k" class=header-anchor >Finding the next search direction \(p_k\)</a></h3> <p>We define the new search direction as \(p_k = -r_k + \beta_kp_{k-1}\). Premultiplying by \(p_{k-1}^TA\) yields</p> \[ p_{k-1}^TAp_k = -p_{k-1}^TAr_k + \beta_kp_{k-1}^TAp_{k-1} \,, \] <p>where the LHS cancels to zero because of conjugacy. Solving for \(\beta_k\) yields</p> \[ \beta_k = \frac{p_{k-1}^TAr_k}{p_{k-1}^TAp_{k-1}} \,. \] <p>With this, we can implement the most basic version of conjugate gradient.</p> <pre><code class=language-julia >function slow_conjgrad&#40;
    A::Matrix&#123;Float64&#125;,  # Constraint matrix
    b::Vector&#123;Float64&#125;,  # Target
    x::Vector&#123;Float64&#125;;  # Initial guess
    tol::Float64&#61;10e-6   # Tolerance for early stop criterion
&#41;
    k &#61; 1
    r &#61; A * x - b
    p &#61; -r
    while norm&#40;r&#41; &gt; tol
        Ap &#61; A * p  # Avoid recomputing a matrix-vector product
        α &#61; -&#40;r&#39; * p&#41; / &#40;p&#39; * Ap&#41;
        x &#61; x &#43; α * p
        r &#61; A * x - b
        β &#61; &#40;r&#39; * Ap&#41; / &#40;p&#39; * Ap&#41;
        p &#61; -r &#43; β * p
        k &#43;&#61; 1
    end
    return x, k
end</code></pre> <h2 id=optimizing_the_conjugate_gradient_method ><a href="#optimizing_the_conjugate_gradient_method" class=header-anchor >Optimizing the conjugate gradient method</a></h2> <p>We can exploit properties of our vectors to make the above algorithm faster.</p> <p>First, we characterize an interesting property of the residuals, \(r_k\). Premulitplying &#40;Eq. 1&#41; by \(A\) and subtracting \(b\) from both sides yields</p> \[ r_{k+1} = r_k + \alpha_kAp_k \,. \quad\quad\text{(Eq. 2)} \] <p>If we look at the first iterate, we see</p> \[ \begin{align*} r_1^Tp_0 &= r_0^T p_0 + \alpha_0p_0^TAp_0 \\ &= r_0^T p_0 + \left(-\frac{p_0^Tr_0}{p_0^TAp_0}\right)p_0^TAp_0 \\ &= \vec 0 \,, \end{align*} \] <p>so, by induction, we can show that \(r_k^Tp_i = 0\) for all \(i=0,\dots,k-1\). That is, the residual is orthogonal to all previous search directions&#33;</p> <h3 id=simplifying_alpha_k ><a href="#simplifying_alpha_k" class=header-anchor >Simplifying \(\alpha_k\)</a></h3> <p>Premultiply the search direction update by the residual, \(r_k \,:\)</p> \[ -p_k^Tr_k = (-r_k)^T(-r_k) + \beta_k(-r_k)^Tp_{k-1} = r_k^Tr_k \,, \] <p>since the residual \(r_k\) and search direction \(p_{k-1}\) are orthogonal. Therefore, we can simplify the calculation of \(\alpha_k \,:\)</p> \[ \alpha_k = -\frac{p_k^Tr_k}{p_k^TAp_k} = \frac{r_k^Tr_k}{p_k^TAp_k} \,. \] <h3 id=simplifying_beta_k ><a href="#simplifying_beta_k" class=header-anchor >Simplifying \(\beta_k\)</a></h3> <p>We get the simplification in two steps</p> <ol> <li><p>\( \alpha_kAp_k = r_{k+1} - r_k \Longrightarrow \alpha_kr_{k+1}^TAp_k = r_{k+1}^Tr_{k+1} + r_{k+1}^Tr_k = r_{k+1}^Tr_{k+1} \) since residuals are mutually orthogonal.</p> <li><p>\( \alpha_k p_k^TAp_k = \left(\frac{r_k^Tr_k}{p_k^TAp_k}\right) p_k^TAp_k = r_k^Tr_k \).</p> </ol> <p>Then,</p> \[ \beta_{k+1} = \frac{p_{k}^TAr_{k+1}}{p_{k}^TAp_{k}} = \frac{\alpha_kp_{k}^TAr_{k+1}}{\alpha_kp_{k}^TAp_{k}} = \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} \,. \] <p>This yields the most widely used form of the conjugate gradient method. We also store a few products at each iteration to optimize the implementation.</p> <pre><code class=language-julia >function fast_conjgrad&#40;
    A::Matrix&#123;Float64&#125;,  # Constraint matrix
    b::Vector&#123;Float64&#125;,  # Target
    x::Vector&#123;Float64&#125;;  # Initial guess
    tol::Float64&#61;10e-6   # Tolerance for early stop criterion
&#41;
    k &#61; 1
    r &#61; b - A * x
    p &#61; r
    rsold &#61; r&#39; * r  # Avoid recomputing a vector-vector product
    while sqrt&#40;rsold&#41; &gt; tol
        Ap &#61; A * p  # Avoid recomputing a matrix-vector product
        α &#61; rsold / &#40;p&#39; * Ap&#41;
        x &#43;&#61; α * p
        r -&#61; α * Ap
        rsnew &#61; r&#39; * r
        if sqrt&#40;rsnew&#41; &lt; tol  # Add an early-stop condition
            break
        end
        β &#61; rsnew / rsold
        p &#61; r &#43; β * p
        rsold &#61; rsnew
        k &#43;&#61; 1
    end
    return x, k
end</code></pre> <h2 id=benchmarking_versions_of_the_conjugate_gradient_method ><a href="#benchmarking_versions_of_the_conjugate_gradient_method" class=header-anchor >Benchmarking versions of the conjugate gradient method</a></h2> <p>Finally, we can benchmark these two versions of the conjugate gradient method. Additionally, we will compare against a barebones implementation of gradient descent with line search:</p> <pre><code class=language-julia >function grad_descent&#40;
    A::Matrix&#123;Float64&#125;,  # Constraint matrix
    b::Vector&#123;Float64&#125;,  # Target
    x::Vector&#123;Float64&#125;;  # Initial guess
    tol::Float64&#61;10e-6   # Tolerance for early stop criterion
&#41;
    k &#61; 1
    r &#61; A * x - b
    rsquared &#61; r&#39; * r  # Avoid recomputing a vector-vector product
    while sqrt&#40;rsquared&#41; &gt; tol
        α &#61; -&#40;rsquared&#41; / &#40;r&#39; * A * r&#41;
        x &#61; x &#43; α * r
        r &#61; A * x - b
        rsquared &#61; r&#39; * r
        k &#43;&#61; 1
    end
    return x, k
end</code></pre> <p>To benchmark these algorithms, we will solve a linear system where the \(A \in M_n\) matrix is the Hilbert matrix,<sup id="fnref:6"><a href="#fndef:6" class=fnref >[6]</a></sup> the target is \(b = (1, \dots, 1)^T\), and the initial guess is \(x = (0, \dots, 0)^T\). We compare the number of iterations required for the slow conjugate gradient, fast conjugate gradient, and standard gradient descent method &#40;\(k_1\), \(k_2\), and \(k_3\) respectively&#41;, as well as the memory requirements, for different matrix dimensions \(n \in \{2, 4, 6\}\).</p> <table class=fndef  id="fndef:6"> <tr> <td class=fndef-backref ><a href="#fnref:6">[6]</a> <td class=fndef-content >That is \((A)_{i,j} = {1}/({i + j - 1}) \,.\) </table> <pre><code class=language-julia >n &#61; 2
κ&#40;A&#40;n&#41;&#41; &#61; 19.28
  595.201 ns &#40;22 allocations: 1.73 KiB&#41;
  446.126 ns &#40;17 allocations: 1.34 KiB&#41;
  5.812 μs &#40;200 allocations: 15.64 KiB&#41;
&#40;k₁, k₂, k₃&#41; &#61; &#40;3, 2, 40&#41;
n &#61; 4
κ&#40;A&#40;n&#41;&#41; &#61; 15513.74
  1.071 μs &#40;38 allocations: 3.66 KiB&#41;
  814.062 ns &#40;31 allocations: 3.00 KiB&#41;
  6.203 ms &#40;201280 allocations: 18.43 MiB&#41;
&#40;k₁, k₂, k₃&#41; &#61; &#40;5, 4, 40256&#41;
n &#61; 6
κ&#40;A&#40;n&#41;&#41; &#61; 1.495105864e7
  2.583 μs &#40;70 allocations: 7.91 KiB&#41;
  1.842 μs &#40;59 allocations: 6.70 KiB&#41;
  5.621 s &#40;136467940 allocations: 14.23 GiB&#41;
&#40;k₁, k₂, k₃&#41; &#61; &#40;9, 8, 27293588&#41;</code></pre> <p>The fast conjugate gradient method is requires fewer iterations to achieve convergence and less memory allocation. Additionally, the standard gradient descent method requires an absurd number of iterations for a poorly conditioned matrix&#33;</p> <p>To get a better comparison between the slow and fast versions of conjugate gradient method, we will use a larger matrix. However, any larger will take gradient descent too long, so we only compare our versions of conjugate gradient with \(n \in \{5, 8, 12, 20 \} \,.\)</p> <pre><code class=language-julia >n &#61; 5
κ&#40;A&#40;n&#41;&#41; &#61; 476607.25
  1.650 μs &#40;54 allocations: 5.22 KiB&#41;
  1.225 μs &#40;45 allocations: 4.38 KiB&#41;
&#40;k₁, k₂&#41; &#61; &#40;7, 6&#41;
n &#61; 8
κ&#40;A&#40;n&#41;&#41; &#61; 1.525756434817e10
  7.417 μs &#40;198 allocations: 25.19 KiB&#41;
  4.238 μs &#40;136 allocations: 17.44 KiB&#41;
&#40;k₁, k₂&#41; &#61; &#40;25, 19&#41;
n &#61; 12
κ&#40;A&#40;n&#41;&#41; &#61; 2.2872145734707476e16
  12.459 μs &#40;262 allocations: 42.00 KiB&#41;
  5.389 μs &#40;143 allocations: 23.41 KiB&#41;
&#40;k₁, k₂&#41; &#61; &#40;33, 20&#41;
n &#61; 20
κ&#40;A&#40;n&#41;&#41; &#61; 4.377151386345373e16
  50.333 μs &#40;638 allocations: 142.59 KiB&#41;
  12.667 μs &#40;234 allocations: 54.22 KiB&#41;
&#40;k₁, k₂&#41; &#61; &#40;80, 33&#41;</code></pre> <p>At these larger sizes of \(A\), the advantage of fast conjugate gradient is clearly appreciable&#33;</p> <h2 id=the_convergence_rate_and_eigenvalue_properties ><a href="#the_convergence_rate_and_eigenvalue_properties" class=header-anchor >The convergence rate and eigenvalue properties</a></h2> <p>TODO :&#41;</p> <div class=page-foot > <div class=copyright > &copy; Septimia Zenobia. Last modified: September 09, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script> <script src="/libs/highlight/highlight.pack.js"></script> <script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: ' '});</script>