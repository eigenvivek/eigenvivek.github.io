{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca0f0ee",
   "metadata": {},
   "source": [
    "# The Conjugate Gradient Method\n",
    "\n",
    "> Notes from the Medical Vision Group's summer reading of _Numerical Optimization. J. Nocedal, Stephen J. Wright, 1999._\n",
    "\n",
    "The goal of this notebook is\n",
    "1. To derive the conjugate gradient method, and \n",
    "2. Explain the connection between eigenvalues and the convergence rate.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The goal of the conjugate gradient algorithm is to iteratively solve linear systems of the form\n",
    "\n",
    "$$Ax = b \\,,$$\n",
    "\n",
    "where **we assume that $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite.**\n",
    "\n",
    "It turns out that solving such a linear system is equivalent to minimizing the quadratic form\n",
    "\n",
    "$$f(x) = x^TAx - b^Tx + c \\,.$$\n",
    "\n",
    "To see this, look at its gradient: \n",
    "\n",
    "$$\\nabla f(x) = Ax - b \\,.$$\n",
    "\n",
    "The gradient equals zero exactly at the $x=x^*$ that minimizes the the residual of the linear system $r(x^*) = Ax^* - b\\,.$\n",
    "(Additionally, the Hessian matrix of $f$ is $A$, which is positive definite, so it has exactly one optimal point!)\n",
    "Since the conjugate gradient method is an iterative one, we will write that the $k$-th iterate $x_k$ has a residual of $r_k = \\nabla f(x_k) = Ax_k - b$.\n",
    "\n",
    "To minimize the quadratic form, we _could_ use steepest descent, which is gradient descent where you take the largest step possible to minimize the loss. \n",
    "However, for certain linear systems, gradient descent converges very slowly (_slowly_ here means that the number of steps needed for convergence is $\\gg n$, the dimension of $A$).\n",
    "Instead, conjugate gradient gives an iterative optimization that is guaranteed to converge in at most $n$ steps (in exact arithmetic; floating point accuracy means it can take a little longer, but it's still faster than gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254dfba",
   "metadata": {},
   "source": [
    "## Understanding conjugacy\n",
    "\n",
    "In essence, the conjugate gradient method is simply a change of basis.\n",
    "However, the basis we seek is very special:\n",
    "specifically, we are looking for a set of basis vectors that are _conjugate_ with respect to $A$.\n",
    "Two vectors $u$ and $v$ are conjugate if they satisfy a special kind of orthogonality:\n",
    "\n",
    "$$ u^T A v = v^T A v = 0 \\,.$$\n",
    "\n",
    "This means that after $u$ (or $v$) is transformed by $A$, it is orthogonal to $v$ (or $u$).\n",
    "This basis is very useful for finding the solution to a linear system, as we'll show below.\n",
    "First, a quick lemma:\n",
    "\n",
    "### Linear independence of conjugate vectors\n",
    "\n",
    "**Lemma 1.** If $u$ and $v$ are conjugate with respect to $A$, then they are also linearly independent.\n",
    "\\\n",
    "_Proof:_ Suppose, by way of contradiction, $u$ and $v$ are not linearly independent. Then, there exists some non-zero constant $k$ such that $u = kv$. This implies $0 = u^TAv = kv^TAv \\implies v^TAv = 0$. However, this is a contradiction because $A$ positive definite means $v^TAv > 0$. $\\square$\n",
    "\n",
    "By induction, we can show that a set of conjugate vectors are linearly independent.\n",
    "\n",
    "### Conjugate directions method\n",
    "\n",
    "Suppose that we have a conjugate basis $\\{p_1, \\dots, p_n\\}$ with respect to $A$.\n",
    "Since these vectors are linearly independent, we can express $x^*$ as\n",
    "\n",
    "$$ x^* = \\alpha_1 p_1 + \\cdots + \\alpha_n p_n \\implies Ax^* = b = \\alpha_1 Ap_1 + \\cdots + \\alpha_n Ap_n \\,. $$\n",
    "\n",
    "Premultiplying by the vector $p_k$, we see that\n",
    "\n",
    "$$ p_k^T b = \\alpha_k p_k^T A p_k \\,, $$\n",
    "\n",
    "since the other terms cancel out by conjugacy! Therefore, we have that the coefficients are\n",
    "\n",
    "$$ \\alpha_k = \\frac{p_k^T b}{p_k^T A p_k} \\,, $$\n",
    "\n",
    "which are all quantities we know how to compute. That is, changing our basis to a conjugate one makes it very easy to solve a linear system.\n",
    "\n",
    "This simple results tells us two important facts:\n",
    "1. If we have a procedure that produces a conjugate basis vector at each step, we can solve a linear system in at most $n$ steps.\n",
    "2. If we have a set of conjugate basis vectors for $A$, it is trivial to solve a linear system. The brilliance of the conjugate gradient method is in how we find these vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d065214",
   "metadata": {},
   "source": [
    "Before discussing the generating procedure, it's useful to visualize conjugacy and our loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266ab73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "\n",
    "using Random\n",
    "Random.seed!(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337f5c3",
   "metadata": {},
   "source": [
    "Since we are working with positive definite matrices, it's useful to have a function to randomly generate them.\n",
    "The procedure we use leverages the fact that $A^T A$ is gauranteed to be a positive semidefinite matrix (use the definition of positive semidefiniteness to prove this). Note that $A^T A$ is positive semidefinite iff $A$ has rank less than $n$, and the probability of this happening is very small. Therefore, we almost always generate a positive definite matrix by samping a random square matrix and premultiplying it by its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbd25d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_psd (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample a random PSD matrix\n",
    "function random_psd(n::Int64=2)\n",
    "    A = randn(n, 2)\n",
    "    return A' * A\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ee9c9",
   "metadata": {},
   "source": [
    "Next, we plot the level sets of the quadratic form defined by our positive definite $A$.\n",
    "That is, we visualize multiple elliptical curves along which $x^TAx = c$ for some constant $c$.\n",
    "These let us visualize the loss landscape we are trying to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d92434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot_ellipse (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the level sets of the isoclines of a PSD matrix\n",
    "function plot_ellipse(A::Matrix{Float64}; t=LinRange(0, 2π, 100), c=:gray)\n",
    "\n",
    "    # Get the trace for the ellipse with radius 1\n",
    "    trace_ellipse(t) = V' * @. sqrt(Λ) * [sin(t) cos(t)]'\n",
    "    Λ, V = eigen(A)\n",
    "    ellipse = trace_ellipse(t)\n",
    "    ellipse_x, ellipse_y = [ellipse[i, :] for i in 1:length(Λ)]\n",
    "\n",
    "    # Draw ellipses with many radii\n",
    "    p = plot(ellipse_x, ellipse_y, c=c, legend=false, aspect_ratio=:equal)\n",
    "    for r in LinRange(1, 1.75, 3)\n",
    "        plot!(p, r .* ellipse_x, r .* ellipse_y, c=c)\n",
    "    end\n",
    "    \n",
    "    # Draw the principle axes\n",
    "    scaled_axes = V .* Λ'\n",
    "    x1, x2 = [scaled_axes[:, i] for i in 1:length(Λ)]\n",
    "    plot!(p, ([0, x1[1]], [0, x1[2]]))\n",
    "    plot!(p, ([0, x2[1]], [0, x2[2]]))\n",
    "    \n",
    "    # Make the title the condition number\n",
    "    λ₁, λ₂ = sort(Λ, rev=true)\n",
    "    κ = λ₁ / λ₂ |> κ -> round(κ; digits=2)\n",
    "    title!(\"κ=$κ\")\n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a51fc6",
   "metadata": {},
   "source": [
    "The final concept we will discuss with these curves is the **condition number**. The condition number is the ratio between the largest and smallest eigenvalues. When this number is small (i.e., closer to 1), the closer the ellipses are to a circle (_left_). This corresponds to a system that is more amenable to gradient descent: you can pick any direction to descent and make good progress. For a system with a large condition number, some directions are much more fruitful than others. This means gradient descent can take a very long time if you choose a poor starting point for the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fd461a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p1 = random_psd(2) |> plot_ellipse\n",
    "p2 = random_psd(2) |> plot_ellipse\n",
    "ellipses = plot(p1, p2, layout=2)\n",
    "savefig(ellipses, \"../images/conjgrad_ellipses.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c1316",
   "metadata": {},
   "source": [
    "## Deriving the conjugate gradient method\n",
    "\n",
    "Here, we derive conjugate gradient as an iterative method.\n",
    "Note, the first search direction $p_0 = -r_0$ since it is the negative gradient.\n",
    "\n",
    "### Finding the optimal step size $\\alpha_k$\n",
    "\n",
    "Assume we start at some point $x_0$.\n",
    "For a set of conjugate directions $\\{p_0, \\dots, p_k\\}$, we define the update function as\n",
    "\n",
    "$$ x_{k+1} = x_k + \\alpha_k p_k \\,, \\tag{*}\\label{eq:update} $$\n",
    "\n",
    "where $\\alpha_k$ is the length that optimally descends $p_k$. To find $\\alpha_k$, we define $g(\\alpha_k) = f(x_k+\\alpha_kp_k)$, so\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial g}{\\partial \\alpha_k} \n",
    "= (x_k + \\alpha_kp_k)^TAp_k - b^Tp_k \n",
    "= x_k^TAp_k + \\alpha_kp_k^TAp_k - b^Tp_k\n",
    "= 0 \\,. $$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\\alpha_k = \\frac{p_k^T(b-Ax_k)}{p_k^TAp_k} = -\\frac{p_k^Tr_k}{p_k^TAp_k} \\,. $$\n",
    "\n",
    "### Finding the next search direction $p_k$\n",
    "\n",
    "We define the new search direction as $p_k = -r_k + \\beta_kp_{k-1}$ (we will prove later that this yields a valid solution).\n",
    "Pre-multiplying by $p_{k-1}^TA$ yields\n",
    "\n",
    "$$ p_{k-1}^TAp_k = -p_{k-1}^TAr_k + \\beta_kp_{k-1}^TAp_{k-1} \\,, $$\n",
    "\n",
    "where the LHS cancels to zero because of conjugacy.\n",
    "Solving for $\\beta_k$ yields\n",
    "\n",
    "$$ \\beta_k = \\frac{p_{k-1}^TAr_k}{p_{k-1}^TAp_{k-1}} \\,. $$\n",
    "\n",
    "With this, we can implement the most basic version of conjugate gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8775a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slow_conjgrad (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function slow_conjgrad(\n",
    "    A::Matrix{Float64},\n",
    "    b::Vector{Float64},\n",
    "    x::Vector{Float64};\n",
    "    tol::Float64=10e-6\n",
    ")\n",
    "    k = 1\n",
    "    r = A * x - b\n",
    "    p = -r\n",
    "    while norm(r) > tol\n",
    "        Ap = A * p  # Avoid recomputing a matrix-vector product\n",
    "        α = -(r' * p) / (p' * Ap)\n",
    "        x = x + α * p\n",
    "        r = A * x - b\n",
    "        β = (r' * Ap) / (p' * Ap)\n",
    "        p = -r + β * p\n",
    "        k += 1\n",
    "    end\n",
    "    return x, k\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d272716",
   "metadata": {},
   "source": [
    "### Optimizing the conjugate gradient method\n",
    "\n",
    "We can exploit properties of our vectors to make the algorithm faster.\n",
    "\n",
    "First, we characterize an interesting property of the residuals, $r_k$.\n",
    "Pre-mulitplying $(*)$ by $A$ and subtracting $b$ from both sides yields\n",
    "\n",
    "$$ r_{k+1} = r_k + \\alpha_kAp_k \\,. \\tag{**} $$\n",
    "\n",
    "If we look at the first iterate, we see\n",
    "\n",
    "\\begin{align*}\n",
    "r_1^Tp_0 \n",
    "&= r_0^T p_0 + \\alpha_0p_0^TAp_0 \\\\\n",
    "&= r_0^T p_0 + \\left(-\\frac{p_0^Tr_0}{p_0^TAp_0}\\right)p_0^TAp_0 \\\\\n",
    "&= \\vec 0 \\,,\n",
    "\\end{align*}\n",
    "\n",
    "so, by induction, we cna show that $r_k^Tp_i = 0$ for all $i=0,\\dots,k-1$.\n",
    "That is, the residual is orthogonal to all previous search directions!\n",
    "\n",
    "#### Simplifying $\\alpha_k$\n",
    "\n",
    "Pre-multiply the search direction update by the residual, $r_k \\,:$\n",
    "\n",
    "$$ -p_k^Tr_k = (-r_k)^T(-r_k) + \\beta_k(-r_k)^Tp_{k-1} = r_k^Tr_k \\,, $$\n",
    "\n",
    "since the residual $r_k$ and search direction $p_{k-1}$ are orthogonal.\n",
    "Therefore, we can simplify the calculation of $\\alpha_k \\,:$ \n",
    "\n",
    "$$\\alpha_k = -\\frac{p_k^Tr_k}{p_k^TAp_k} = \\frac{r_k^Tr_k}{p_k^TAp_k} \\,. $$\n",
    "\n",
    "#### Simplifying $\\beta_k$\n",
    "\n",
    "We get the simplification in two steps\n",
    "\n",
    "1. $\\alpha_kAp_k = r_{k+1} - r_k \\implies \\alpha_kr_{k+1}^TAp_k = r_{k+1}^Tr_{k+1} + r_{k+1}^Tr_k = r_{k+1}^Tr_{k+1}$ since residuals are mutually orthogonal.\n",
    "2. $\\alpha_k p_k^TAp_k = \\left(\\frac{r_k^Tr_k}{p_k^TAp_k}\\right) p_k^TAp_k = r_k^Tr_k$.\n",
    "\n",
    "Then,\n",
    "\n",
    "$$ \\beta_{k+1} = \\frac{p_{k}^TAr_{k+1}}{p_{k}^TAp_{k}} = \\frac{\\alpha_kp_{k}^TAr_{k+1}}{\\alpha_kp_{k}^TAp_{k}} = \\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeebe21",
   "metadata": {},
   "source": [
    "This yields the most widely used form of the conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e95589e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fast_conjgrad (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fast_conjgrad(A::Matrix{Float64}, b::Vector{Float64}, x::Vector{Float64}; tol::Float64=10e-6)\n",
    "    k = 1\n",
    "    r = b - A * x\n",
    "    p = r\n",
    "    rsold = r' * r  # Avoid recomputing a vector-vector product\n",
    "    while sqrt(rsold) > tol\n",
    "        Ap = A * p  # Avoid recomputing a matrix-vector product\n",
    "        α = rsold / (p' * Ap)\n",
    "        x += α * p\n",
    "        r -= α * Ap\n",
    "        rsnew = r' * r\n",
    "        if sqrt(rsnew) < tol  # Add an early-stop condition\n",
    "            break\n",
    "        end\n",
    "        β = rsnew / rsold\n",
    "        p = r + β * p\n",
    "        rsold = rsnew\n",
    "        k += 1\n",
    "    end\n",
    "    return x, k\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa5682a",
   "metadata": {},
   "source": [
    "Finally, we can benchmark these two versions of the conjugate gradient method.\n",
    "Additionally, we will compare against a barebones implementation of gradient descent with line search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ef4f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function grad_descent(\n",
    "    A::Matrix{Float64},  # Constraint matrix\n",
    "    b::Vector{Float64},  # Target\n",
    "    x::Vector{Float64};  # Initial guess\n",
    "    tol::Float64=10e-6   # Tolerance for early stop criterion\n",
    ")\n",
    "    k = 1\n",
    "    r = A * x - b\n",
    "    rsquared = r' * r  # Avoid recomputing a vector-vector product\n",
    "    while sqrt(rsquared) > tol\n",
    "        α = -(rsquared) / (r' * A * r)\n",
    "        x = x + α * r\n",
    "        r = A * x - b\n",
    "        rsquared = r' * r\n",
    "        k += 1\n",
    "    end\n",
    "    return x, k\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7622a",
   "metadata": {},
   "source": [
    "Our coefficient matrix is a Hilbert matrix and the target is ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbeb8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "κ (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cond \n",
    "function κ(A::Matrix{Float64})\n",
    "    Λ = eigvals(A, sortby=max)\n",
    "    λ₁, λₙ = first(Λ), last(Λ)\n",
    "    return λₙ / λ₁ |> abs |> x -> round(x; digits=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51591faa",
   "metadata": {},
   "source": [
    "The fast conjugate gradient method is requires fewer iterations to achieve convergence and less memory allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b6854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 2\n",
      "κ(A(n)) = 19.28\n",
      "  595.201 ns (22 allocations: 1.73 KiB)\n",
      "  446.126 ns (17 allocations: 1.34 KiB)\n",
      "  5.812 μs (200 allocations: 15.64 KiB)\n",
      "(k₁, k₂, k₃) = (3, 2, 40)\n",
      "n = 4\n",
      "κ(A(n)) = 15513.74\n",
      "  1.071 μs (38 allocations: 3.66 KiB)\n",
      "  814.062 ns (31 allocations: 3.00 KiB)\n",
      "  6.203 ms (201280 allocations: 18.43 MiB)\n",
      "(k₁, k₂, k₃) = (5, 4, 40256)\n",
      "n = 6\n",
      "κ(A(n)) = 1.495105864e7\n",
      "  2.583 μs (70 allocations: 7.91 KiB)\n",
      "  1.842 μs (59 allocations: 6.70 KiB)\n",
      "  5.621 s (136467940 allocations: 14.23 GiB)\n",
      "(k₁, k₂, k₃) = (9, 8, 27293588)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "\n",
    "A(n::Int64) = [1 / (i + j - 1) for i=1:n, j=1:n]  # Hilbert matrix\n",
    "b(n::Int64) = ones(n)                             # Target\n",
    "x(n::Int64) = zeros(n)                            # Initial guess\n",
    "\n",
    "for n in [2, 4, 6]\n",
    "    @show n\n",
    "    @show κ(A(n))\n",
    "    _, k₁ = @btime slow_conjgrad(A($n), b($n), x($n))\n",
    "    _, k₂ = @btime fast_conjgrad(A($n), b($n), x($n))\n",
    "    _, k₃ = @btime grad_descent(A($n), b($n), x($n))\n",
    "    @show k₁, k₂, k₃\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcdf72fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 5\n",
      "κ(A(n)) = 476607.25\n",
      "  1.650 μs (54 allocations: 5.22 KiB)\n",
      "  1.225 μs (45 allocations: 4.38 KiB)\n",
      "(k₁, k₂) = (7, 6)\n",
      "n = 8\n",
      "κ(A(n)) = 1.525756434817e10\n",
      "  7.417 μs (198 allocations: 25.19 KiB)\n",
      "  4.238 μs (136 allocations: 17.44 KiB)\n",
      "(k₁, k₂) = (25, 19)\n",
      "n = 12\n",
      "κ(A(n)) = 2.2872145734707476e16\n",
      "  12.459 μs (262 allocations: 42.00 KiB)\n",
      "  5.389 μs (143 allocations: 23.41 KiB)\n",
      "(k₁, k₂) = (33, 20)\n",
      "n = 20\n",
      "κ(A(n)) = 4.377151386345373e16\n",
      "  50.333 μs (638 allocations: 142.59 KiB)\n",
      "  12.667 μs (234 allocations: 54.22 KiB)\n",
      "(k₁, k₂) = (80, 33)\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 8, 12, 20]\n",
    "    @show n\n",
    "    @show κ(A(n))\n",
    "    _, k₁ = @btime slow_conjgrad(A($n), b($n), x($n))\n",
    "    _, k₂ = @btime fast_conjgrad(A($n), b($n), x($n))\n",
    "    @show k₁, k₂\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c28029",
   "metadata": {},
   "source": [
    "## Convergence rate\n",
    "\n",
    "- Matrix polynomials\n",
    "- Condition number\n",
    "- Eigenvalue clusters (and the cool concept of intra-cluster condition number!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e08668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
